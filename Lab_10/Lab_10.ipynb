{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Load the required libraries"
      ],
      "metadata": {
        "id": "RB-s-A3CmM-4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tk6Pmc3UmGd_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- load the data,<br>\n",
        "  when using pd.read_csv, make sure to specify:<br>\n",
        "  na_values=['?']<br>\n",
        "  as the data has many missing values coded as '?'<br>\n",
        "  this setting will turn them into NaNs, which makes subsequent processing a lot easier<br>\n",
        "  split train.csv into X_train and y_train, using the last feature 'Class' for y_train<br>\n",
        "  split test.csv the same way"
      ],
      "metadata": {
        "id": "cFfWw1jnoB7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data with missing values treated as NaN\n",
        "train_data = pd.read_csv('https://raw.githubusercontent.com/bpfa/data_for_compx310_2023/main/lab10/train.csv', na_values=['?'])\n",
        "test_data = pd.read_csv('https://raw.githubusercontent.com/bpfa/data_for_compx310_2023/main/lab10/test.csv', na_values=['?'])\n",
        "\n",
        "# Split train data into X_train and y_train\n",
        "X_train = train_data.drop('Class', axis=1)  # X_train contains all columns except 'Class'\n",
        "y_train = train_data['Class']\n",
        "\n",
        "# Split test data the same way\n",
        "X_test = test_data.drop('Class', axis=1)  # X_test contains all columns except 'Class'\n",
        "y_test = test_data['Class']"
      ],
      "metadata": {
        "id": "twYfLBPtoKsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  this time we want to use pipelines for all processing, to make sure\n",
        "  that train and test (and validation splits and cross-validation)\n",
        "  all see correctly processed data in the same way\n",
        "\n",
        "  So will need need to setup one pipeline for categorical features\n",
        "  combining a SimpleImputer using a constant value of \"missing\"\n",
        "  with a OneHotEncoder\n",
        "  \n",
        "  one pipeline for numeric features\n",
        "  combining a SimpleImputer using the \"mean\" for imputation\n",
        "  with a StandardScalar\n",
        "\n",
        "  these two pipelines can be put together using a\n",
        "  sklearn.compose.ColumnTransformer\n",
        "\n",
        "  finally you will then combine this preprocesser\n",
        "  with different classifiers, again using a pipeline (see below)\n"
      ],
      "metadata": {
        "id": "c-MUzZ_9pXzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_features = X_train.select_dtypes(exclude=['object']).columns.tolist()\n",
        "\n",
        "# setup one pipeline for categorical features combining a SimpleImputer using a constant value of \"missing\" with a OneHotEncoder\n",
        "categorical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# setup one pipeline for numeric features combining a SimpleImputer using the \"mean\" for imputation with a StandardScaler\n",
        "numerical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Define a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_pipeline, numerical_features),\n",
        "        ('cat', categorical_pipeline, categorical_features)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "O-Js-WH5rdhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- define a function repeated_tt(X, y, classifier, num_repeats, random_seed, val_fraction)\n",
        "\n",
        "  This function will split X into train + val (using sklearn.model_selection.train_test_split) num_repeats times, fit the classifier on train and use accuracy_score to compute the accuracy on the val set<br>\n",
        "  this function will return the mean value of all num_repeats validation accuracies\n",
        "  <br>\n",
        "  to be sure that you get different random splits each time, always specify\n",
        "  random_state=random_seed + i, where i is in range(0, num_repeats)"
      ],
      "metadata": {
        "id": "NNV6-mqQtW8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with which to convert numpy arrays to DataFrames\n",
        "def convert_to_dataframe(X):\n",
        "    return pd.DataFrame(X)\n",
        "\n",
        "# define a function repeated_tt(X, y, classifier, num_repeats, random_seed, val_fraction)\n",
        "def repeated_tt(X, y, classifier, num_repeats, random_seed, val_fraction):\n",
        "    accuracy_scores = []\n",
        "\n",
        "    # This function will split X into train + val (using sklearn.model_selection.train_test_split) num_repeats times\n",
        "    for i in range(num_repeats):\n",
        "\n",
        "        # to be sure that you get different random splits each time, always specify random_state=random_seed + i, where i is in range(0, num_repeats)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_fraction, random_state=random_seed + i)\n",
        "\n",
        "        # Convert the transformed arrays back to DataFrames for the classifier\n",
        "        X_train_df = convert_to_dataframe(X_train)\n",
        "        X_val_df = convert_to_dataframe(X_val)\n",
        "\n",
        "        # fit the classifier on train\n",
        "        pipe = Pipeline([('clf', classifier)])\n",
        "        pipe.fit(X_train_df, y_train)\n",
        "        y_pred = pipe.predict(X_val_df)\n",
        "\n",
        "        # use accuracy_score to compute the accuracy on the val set\n",
        "        accuracy = accuracy_score(y_val, y_pred)\n",
        "        accuracy_scores.append(accuracy)\n",
        "\n",
        "    # this function will return the mean value of all num_repeats validation accuracies\n",
        "    return np.mean(accuracy_scores)\n"
      ],
      "metadata": {
        "id": "1TLtdEemvUsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- define a function repeated_cv10(X, y, classifier, num_repeats, random_seed)<br>\n",
        "  This function will use sklearn.model_selection.cross_val_score to run 10-fold cross-validation num_repeats times<br>\n",
        "  cross_val_score returns 10 accuracy scores that you should average into one value<br>\n",
        "  This function will return the mean accuracy over the num_repeats 10-fold cv accuracy means, ie. a mean of means<br>\n",
        "  cross_val_score does NOT shuffle the data automatically, so you will need to use sklearn.model_selection.StratifiedKFold to specify data shuffling explicitly as an argument to cross_val_score(..., cv = StratifiedKFold(10, True, random_seed+i) ...) where again i is in range(0, num_repeats)"
      ],
      "metadata": {
        "id": "OFcggRkSxSPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function repeated_cv10(X, y, classifier, num_repeats, random_seed)\n",
        "def repeated_cv10(X, y, classifier, num_repeats, random_seed):\n",
        "    accuracy_scores = []\n",
        "\n",
        "    # use sklearn.model_selection.StratifiedKFold to specify data shuffling explicitly as an argument to cross_val_score\n",
        "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_seed)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(num_repeats):\n",
        "        # Convert X to a DataFrame\n",
        "        X = convert_to_dataframe(X)\n",
        "\n",
        "        # Convert the transformed array back to a DataFrame for the classifier\n",
        "        X_df = convert_to_dataframe(X)\n",
        "\n",
        "        pipe = Pipeline([('clf', classifier)])\n",
        "\n",
        "        # This function will use sklearn.model_selection.cross_val_score to run 10-fold cross-validation num_repeats times\n",
        "        scores = cross_val_score(pipe, X_df, y, cv=skf)\n",
        "        accuracy_scores.append(np.mean(scores))\n",
        "\n",
        "    # cross_val_score returns 10 accuracy scores that you should average into one value\n",
        "    return np.mean(accuracy_scores)\n"
      ],
      "metadata": {
        "id": "Ke5-1JrUuyVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- then you will compare a diverse set of classifiers:<br>\n",
        "  LogisticRegression (with default settings)<br>\n",
        "  RandomForestClassifier with max_features in [5, \"sqrt\", 25]<br>\n",
        "  GaussianNB (with default settings)<br>\n",
        "  KNeighborsClassifier with n_neighbors in [1, 5, 25]<br><br>\n",
        "  for these 8 classifiers, always setup a pipeline<br>\n",
        "  pipe_XX = Pipeline([('preproc', my_column_transformer), ('XX', XX(...))])<br>\n",
        "  where XX is the respective classifier,<br> e.g.\n",
        "  pipe_rf_5 = Pipeline([('preproc', my_column_transformer), ('rf_5', RandomForestClassifier(max_features=5))])"
      ],
      "metadata": {
        "id": "8wTRYWI8Rgtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare a diverse set of classifiers\n",
        "classifiers = {\n",
        "    # LogisticRegression (with default settings)\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "\n",
        "    # RandomForestClassifier with max_features in [5, \"sqrt\", 25]\n",
        "    'Random Forest (max_features=5)': RandomForestClassifier(max_features=5),\n",
        "    'Random Forest (max_features=sqrt)': RandomForestClassifier(max_features='sqrt'),\n",
        "    'Random Forest (max_features=25)': RandomForestClassifier(max_features=25),\n",
        "\n",
        "    # GaussianNB (with default settings)\n",
        "    'Gaussian Naive Bayes': GaussianNB(),\n",
        "\n",
        "    # KNeighborsClassifier with n_neighbors in [1, 5, 25]\n",
        "    'K-Nearest Neighbors (n_neighbors=1)': KNeighborsClassifier(n_neighbors=1),\n",
        "    'K-Nearest Neighbors (n_neighbors=5)': KNeighborsClassifier(n_neighbors=5),\n",
        "    'K-Nearest Neighbors (n_neighbors=25)': KNeighborsClassifier(n_neighbors=25)\n",
        "}\n",
        "\n",
        "# for these 8 classifiers, always setup a pipeline\n",
        "classifier_pipelines = {}\n",
        "for classifier_name, classifier in classifiers.items():\n",
        "    pipe_name = f'pipe_{classifier_name.lower().replace(\" \", \"_\")}'\n",
        "    classifier_pipeline = Pipeline([\n",
        "        ('preproc', preprocessor),\n",
        "        ('classifier', classifier)\n",
        "    ])\n",
        "    classifier_pipelines[pipe_name] = classifier_pipeline\n"
      ],
      "metadata": {
        "id": "K9rFu2rgR0I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> for each of the 8 classifier pipelines compute 5 different accuracy estimates:\n",
        "\n",
        "  >> a) repeated_tt(X_train, y_train, pipe_XX, 1, YOUR_ID, 0.25)\n",
        "  \n",
        "  >> b) repeated_tt(X_train, y_train, pipe_XX, 100, YOUR_ID, 0.25)\n",
        "\n",
        "  >> c) repeated_cv(X_train, y_train, pipe_XX, 1, YOUR_ID)\n",
        "  \n",
        "  >> d) repeated_cv(X_train, y_train, pipe_XX, 10, YOUR_ID)\n",
        "\n",
        "  >> e) accuracy_score(y_test, pipe_XX.fit(X_train, y_train).predict(X_test))\n",
        "\n",
        "  > Produce one summary table with all these 8*5=40 accuracy scores\n",
        "  using one row per classifier pipeline, with 5 columns each\n",
        "  in each column highlight the highest accuracy (use bold, or some colour)"
      ],
      "metadata": {
        "id": "sKTWQRNxTEWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "results = {}\n",
        "\n",
        "MY_ID = 1481257\n",
        "\n",
        "# for each of the 8 classifier pipelines\n",
        "for pipe_name, pipeline in classifier_pipelines.items():\n",
        "    # compute 5 different accuracy estimate\n",
        "    results[pipe_name] = [\n",
        "\n",
        "        # a) repeated_tt(X_train, y_train, pipe_XX, 1, YOUR_ID, 0.25)\n",
        "        repeated_tt(X_train, y_train, pipeline, 1, MY_ID, 0.25),\n",
        "\n",
        "        # b) repeated_tt(X_train, y_train, pipe_XX, 100, YOUR_ID, 0.25)\n",
        "        repeated_tt(X_train, y_train, pipeline, 100, MY_ID, 0.25),\n",
        "\n",
        "        # c) repeated_cv(X_train, y_train, pipe_XX, 1, YOUR_ID)\n",
        "        repeated_cv10(X_train, y_train, pipeline, 1, MY_ID),\n",
        "\n",
        "        # d) repeated_cv(X_train, y_train, pipe_XX, 10, YOUR_ID)\n",
        "        repeated_cv10(X_train, y_train, pipeline, 10, MY_ID),\n",
        "\n",
        "        # e) accuracy_score(y_test, pipe_XX.fit(X_train, y_train).predict(X_test))\n",
        "        accuracy_score(y_test, pipeline.fit(X_train, y_train).predict(X_test))\n",
        "    ]\n",
        "\n",
        "# Create a DataFrame to display the results\n",
        "result_df = pd.DataFrame(results, index=['Single TT', 'Repeated(100) TT', 'Single CV', 'Repeated(10) CV', 'Test Set']).T\n",
        "\n",
        "# Highlight the highest accuracy values in the DataFrame\n",
        "def highlight_max(s):\n",
        "    is_max = s == s.max()\n",
        "    return ['background-color: yellow' if v else '' for v in is_max]\n",
        "\n",
        "# Produce one summary table with all these 8*5=40 accuracy scores using one row per classifier pipeline,\n",
        "# with 5 columns each in each column highlight the highest accuracy\n",
        "result_df.style.apply(highlight_max)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "voqz6apOTkCY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "02e5117f-1bfc-4a52-9544-2cdf63c3aefa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x78a4aee14580>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_d37c5_row3_col0, #T_d37c5_row3_col1, #T_d37c5_row3_col2, #T_d37c5_row3_col3, #T_d37c5_row3_col4 {\n",
              "  background-color: yellow;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_d37c5\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_d37c5_level0_col0\" class=\"col_heading level0 col0\" >Single TT</th>\n",
              "      <th id=\"T_d37c5_level0_col1\" class=\"col_heading level0 col1\" >Repeated(100) TT</th>\n",
              "      <th id=\"T_d37c5_level0_col2\" class=\"col_heading level0 col2\" >Single CV</th>\n",
              "      <th id=\"T_d37c5_level0_col3\" class=\"col_heading level0 col3\" >Repeated(10) CV</th>\n",
              "      <th id=\"T_d37c5_level0_col4\" class=\"col_heading level0 col4\" >Test Set</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_d37c5_level0_row0\" class=\"row_heading level0 row0\" >pipe_logistic_regression</th>\n",
              "      <td id=\"T_d37c5_row0_col0\" class=\"data row0 col0\" >0.961589</td>\n",
              "      <td id=\"T_d37c5_row0_col1\" class=\"data row0 col1\" >0.965166</td>\n",
              "      <td id=\"T_d37c5_row0_col2\" class=\"data row0 col2\" >0.966854</td>\n",
              "      <td id=\"T_d37c5_row0_col3\" class=\"data row0 col3\" >0.966854</td>\n",
              "      <td id=\"T_d37c5_row0_col4\" class=\"data row0 col4\" >0.972185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d37c5_level0_row1\" class=\"row_heading level0 row1\" >pipe_random_forest_(max_features=5)</th>\n",
              "      <td id=\"T_d37c5_row1_col0\" class=\"data row1 col0\" >0.978808</td>\n",
              "      <td id=\"T_d37c5_row1_col1\" class=\"data row1 col1\" >0.982543</td>\n",
              "      <td id=\"T_d37c5_row1_col2\" class=\"data row1 col2\" >0.983100</td>\n",
              "      <td id=\"T_d37c5_row1_col3\" class=\"data row1 col3\" >0.983366</td>\n",
              "      <td id=\"T_d37c5_row1_col4\" class=\"data row1 col4\" >0.972185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d37c5_level0_row2\" class=\"row_heading level0 row2\" >pipe_random_forest_(max_features=sqrt)</th>\n",
              "      <td id=\"T_d37c5_row2_col0\" class=\"data row2 col0\" >0.974834</td>\n",
              "      <td id=\"T_d37c5_row2_col1\" class=\"data row2 col1\" >0.984013</td>\n",
              "      <td id=\"T_d37c5_row2_col2\" class=\"data row2 col2\" >0.984758</td>\n",
              "      <td id=\"T_d37c5_row2_col3\" class=\"data row2 col3\" >0.984194</td>\n",
              "      <td id=\"T_d37c5_row2_col4\" class=\"data row2 col4\" >0.976159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d37c5_level0_row3\" class=\"row_heading level0 row3\" >pipe_random_forest_(max_features=25)</th>\n",
              "      <td id=\"T_d37c5_row3_col0\" class=\"data row3 col0\" >0.982781</td>\n",
              "      <td id=\"T_d37c5_row3_col1\" class=\"data row3 col1\" >0.987351</td>\n",
              "      <td id=\"T_d37c5_row3_col2\" class=\"data row3 col2\" >0.989067</td>\n",
              "      <td id=\"T_d37c5_row3_col3\" class=\"data row3 col3\" >0.988338</td>\n",
              "      <td id=\"T_d37c5_row3_col4\" class=\"data row3 col4\" >0.980132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d37c5_level0_row4\" class=\"row_heading level0 row4\" >pipe_gaussian_naive_bayes</th>\n",
              "      <td id=\"T_d37c5_row4_col0\" class=\"data row4 col0\" >0.340397</td>\n",
              "      <td id=\"T_d37c5_row4_col1\" class=\"data row4 col1\" >0.290318</td>\n",
              "      <td id=\"T_d37c5_row4_col2\" class=\"data row4 col2\" >0.321847</td>\n",
              "      <td id=\"T_d37c5_row4_col3\" class=\"data row4 col3\" >0.321847</td>\n",
              "      <td id=\"T_d37c5_row4_col4\" class=\"data row4 col4\" >0.356291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d37c5_level0_row5\" class=\"row_heading level0 row5\" >pipe_k-nearest_neighbors_(n_neighbors=1)</th>\n",
              "      <td id=\"T_d37c5_row5_col0\" class=\"data row5 col0\" >0.958940</td>\n",
              "      <td id=\"T_d37c5_row5_col1\" class=\"data row5 col1\" >0.963007</td>\n",
              "      <td id=\"T_d37c5_row5_col2\" class=\"data row5 col2\" >0.963210</td>\n",
              "      <td id=\"T_d37c5_row5_col3\" class=\"data row5 col3\" >0.963210</td>\n",
              "      <td id=\"T_d37c5_row5_col4\" class=\"data row5 col4\" >0.960265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d37c5_level0_row6\" class=\"row_heading level0 row6\" >pipe_k-nearest_neighbors_(n_neighbors=5)</th>\n",
              "      <td id=\"T_d37c5_row6_col0\" class=\"data row6 col0\" >0.965563</td>\n",
              "      <td id=\"T_d37c5_row6_col1\" class=\"data row6 col1\" >0.967205</td>\n",
              "      <td id=\"T_d37c5_row6_col2\" class=\"data row6 col2\" >0.970173</td>\n",
              "      <td id=\"T_d37c5_row6_col3\" class=\"data row6 col3\" >0.970173</td>\n",
              "      <td id=\"T_d37c5_row6_col4\" class=\"data row6 col4\" >0.970861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d37c5_level0_row7\" class=\"row_heading level0 row7\" >pipe_k-nearest_neighbors_(n_neighbors=25)</th>\n",
              "      <td id=\"T_d37c5_row7_col0\" class=\"data row7 col0\" >0.954967</td>\n",
              "      <td id=\"T_d37c5_row7_col1\" class=\"data row7 col1\" >0.958437</td>\n",
              "      <td id=\"T_d37c5_row7_col2\" class=\"data row7 col2\" >0.959564</td>\n",
              "      <td id=\"T_d37c5_row7_col3\" class=\"data row7 col3\" >0.959564</td>\n",
              "      <td id=\"T_d37c5_row7_col4\" class=\"data row7 col4\" >0.969536</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  >   Which of the four \"estimators\":\n",
        "*   1 repeated_tt\n",
        "*   100 repeated_tt\n",
        "*   1 repeated_cv\n",
        "*   10 repeat_cv\n",
        "<br><br>\n",
        "  is most reliably identifying the\n",
        "  method with the best \"test\" accuracy?\n",
        "\n",
        "  > **Answer:**\n",
        "  >> The pipe_random_forest_(max_features=25) classifier exhibits the best accuracy scores, so I will be considering this classifier as I answer this question.\n",
        "\n",
        ">>The \"Repeated(10) CV\" estimator (0.988602) has the highest accuracy, making it the most reliable in identifying the method with the best \"test\" accuracy for this classifier.\n",
        "<br><br><br>\n",
        ">>The \"Single CV\" estimator (0.988074) also has the second highest accuracy, which makes it quite reliable.\n",
        "\n",
        ">>The \"Repeated(100) TT\" estimator (0.987245) follows at third most accurate, indicating good accuracy but slightly less reliability compared to the CV estimators.\n",
        "\n",
        ">>The \"Single TT\" estimator (0.980132\t) is the fourth most accurate, making it less reliable than the other three but more reliable than the \"Test Set\" accuracy (0.978808).\n",
        "<br><br>\n",
        ">>In conclusion, of the four estimators, the \"Repeated(10) CV\" estimator is most reliably identifying the method with the best \"test\" accuracy."
      ],
      "metadata": {
        "id": "6mQ1-kABZjNb"
      }
    }
  ]
}